---
# run with:
# cd /opt/meza/ansible
# sudo -u meza-ansible ansible-playbook site.yml

# FIXME #729: Move this into a role
- hosts: localhost
  become: yes
  vars:
    m_home: "/opt/conf-meza/users"
  tasks:
  - name: Ensure no password on meza-ansible user on controller
    shell: passwd --delete meza-ansible
    failed_when: False

  - name: Ensure controller has user alt-meza-ansible
    user:
      name: alt-meza-ansible
      move_home: yes
      home: "{{ m_home }}/alt-meza-ansible"
      # primary group
      group: wheel

  - name: Ensure user alt-meza-ansible .ssh dir configured
    file:
      path: "{{ m_home }}/alt-meza-ansible/.ssh"
      owner: alt-meza-ansible
      group: wheel
      mode: 0700
      state: directory

  - name: Copy meza-ansible keys to alt-meza-ansible
    copy:
      src: "{{ m_home }}/meza-ansible/.ssh/{{ item.name }}"
      dest: "{{ m_home }}/alt-meza-ansible/.ssh/{{ item.name }}"
      owner: alt-meza-ansible
      group: wheel
      mode: "{{ item.mode }}"
    with_items:
    - name: id_rsa
      mode: "0600"
    - name: id_rsa.pub
      mode: "0644"

  - name: Copy meza-ansible known_hosts to alt-meza-ansible
    copy:
      src: "{{ m_home }}/meza-ansible/.ssh/{{ item.name }}"
      dest: "{{ m_home }}/alt-meza-ansible/.ssh/{{ item.name }}"
      owner: alt-meza-ansible
      group: wheel
      mode: "{{ item.mode }}"
    failed_when: False
    with_items:
    - name: known_hosts
      mode: "0600"

  # Note: without this, if the user decides to encrypt secret.yml with Ansible
  # vault then it changes mode to 0600 and ownership to root:root. This makes it
  # impossible to include_vars later.
  - name: Ensure secret.yml owned by meza-ansible
    file:
      path: "/opt/conf-meza/secret/{{ env }}/secret.yml"
      owner: meza-ansible
      group: wheel
      mode: "0600"

# FIXME 800: Run against localhost
- hosts: app-servers
  become: yes
  roles:
    - set-vars
    - umask-set
    - init-controller-config

- hosts: localhost
  become: yes
  roles:
    - set-vars
    - role: autodeployer
      when: force_deploy is defined or autodeployer is defined

# Ensure proper base setup on all servers in inventory, with the exception of
# servers in "exclude-all" group. At present, the intent of this group is to
# allow servers which serve as sources for database and user-uploaded files,
# but are not managed by this meza install.
- hosts: all:!exclude-all:!load-balancers-nonmeza:!load-balancers-nonmeza-external:!load-balancers-nonmeza-internal
  become: yes
  roles:
    - set-vars
    - umask-set
    - base
    - base-config-scripts
  tags: base

- hosts: load-balancers,load-balancers-meza-internal,load-balancers-meza-external
  become: yes
  tags:
    - load-balancer
    - load-balancers
    - haproxy
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 8001
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 8081
      firewall_protocol: tcp
      firewall_servers: "{{ groups['parsoid-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    # NOTE: firewalld ports 80 and 443 opened to ALL within haproxy role
    - haproxy

- hosts: app-servers
  become: yes
  tags: apache-php
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 8080
      firewall_protocol: tcp
      firewall_servers: "{{ load_balancers_all }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 8080
      firewall_protocol: tcp
      firewall_servers: "{{ groups['parsoid-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - base-extras
    - imagemagick
    - apache-php
    - role: netdata
      when: m_install_netdata
    - role: firewall_port
      firewall_action: open
      firewall_port: 19999
      firewall_protocol: tcp
      firewall_servers: "{{ load_balancers_all }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"

# Keeping this as a separate play, since at some point it will make sense to
# configure app-servers as gluster clients and have separate gluster servers
# (which can also be app-servers, of course, like everything else in meza)
- hosts: app-servers
  become: yes
  tags: gluster
  vars:
    gluster_replicas: "{{ groups['app-servers'] | length }}"
  roles:
    - set-vars

    # Portmap tcp and udp
    #
    # From docs:
    # ref: http://gluster.readthedocs.io/en/latest/Administrator%20Guide/Troubleshooting/?highlight=38465
    #
    # Check your firewall setting to open ports 111 for portmap
    # requests/replies and Gluster NFS server requests/replies. Gluster NFS
    # server operates over the following port numbers: 38465, 38466, and 38467.
    - role: firewall_port
      firewall_action: open
      firewall_port: 111
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"
    - role: firewall_port
      firewall_action: open
      firewall_port: 111
      firewall_protocol: udp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"

    # All gluster servers, Gluster Daemon
    # maybe also: 24009, 24010, 24011
    #
    # From the docs
    # ref: https://gluster.readthedocs.io/en/latest/Administrator%20Guide/Setting%20Up%20Clients/
    #
    # Ensure that TCP and UDP ports 24007 and 24008 are open on all Gluster
    # servers. Apart from these ports, you need to open one port for each brick
    # starting from port 49152 (instead of 24009 onwards as with previous
    # releases). The brick ports assignment scheme is now compliant with IANA
    # guidelines. For example: if you have five bricks, you need to have ports
    # 49152 to 49156 open.
    - role: firewall_port
      firewall_action: open
      firewall_port: 24007
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"
    - role: firewall_port
      firewall_action: open
      firewall_port: 24008
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"

    # One port required for each brick (in meza, this equals each server (one
    # brick per server))
    - role: firewall_port
      firewall_action: open
      firewall_port: 49152
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"
    - role: firewall_port
      firewall_action: open
      firewall_port: 49153
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"
    - role: firewall_port
      firewall_action: open
      firewall_port: 49154
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"
    - role: firewall_port
      firewall_action: open
      firewall_port: 49155
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: "groups['app-servers']|length|int > 1"

    # FIXME #801: Some docs said ports 38465, 38466, 38467 needed for Gluster

    - role: gluster
      when: "groups['app-servers']|length|int > 1"


- hosts: memcached-servers
  become: yes
  tags: memcached
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 11211
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - memcached

- hosts: db-master
  become: yes
  tags: database
  roles:
    - set-vars
    - role: firewall_service
      firewall_service: mysql
      # FIXME: try merging these two mysql lines with:
      # firewall_servers: "{{ groups['app-servers'] }} + {{ groups['db-slaves'] }}"
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_service
      firewall_service: mysql
      firewall_servers: "{{ groups['db-slaves'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"

    - role: database
      # Get the one and only server that should be in the db-master group and set
      # it's IP address as replication master IP. Note that this can be left blank
      # or not included at all if no replication should be performed.
      # FIXME #802: Should replication master be set only if slaves exist?
      mysql_replication_master: "{{ groups['db-master'][0] }}"
      mysql_replication_role: master


- hosts: db-slaves
  become: yes
  tags: database
  roles:
    - set-vars
    - role: firewall_service
      firewall_service: mysql
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
      when: ansible_os_family == 'RedHat'
    - role: database
      # Get the one and only server that should be in the db-master group and set
      # it's IP address as replication master IP. Note that this can be left blank
      # or not included at all if no replication should be performed.
      mysql_replication_master: "{{ groups['db-master'][0] }}"
      mysql_replication_role: slave


- hosts: elastic-servers
  become: yes
  tags: elasticsearch
  roles:
    - set-vars
    - role: firewall_port
      firewall_action: open
      firewall_port: 9200
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 9300
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 9200
      firewall_protocol: tcp
      firewall_servers: "{{ groups['elastic-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 9300
      firewall_protocol: tcp
      firewall_servers: "{{ groups['elastic-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - elasticsearch

# Note: this is app-servers again, but must be after everything else is setup
- hosts: app-servers
  become: yes
  tags: mediawiki
  roles:
    - set-vars
    - htdocs
    - role: lua
      when: m_install_lua
    - mediawiki


# Parsoid configuration moved after MediaWiki since Parsoid needs to know which
# wikis exist before writing localsettings.js and (re)starting the service
- hosts: parsoid-servers
  become: yes
  tags: parsoid
  roles:
    - set-vars
    # Allow app servers to get to parsoid server(s) on port 8000
    - role: firewall_port
      firewall_action: open
      firewall_port: 8000
      firewall_protocol: tcp
      firewall_servers: "{{ groups['app-servers'] }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - role: firewall_port
      firewall_action: open
      firewall_port: 8000
      firewall_protocol: tcp
      firewall_servers: "{{ load_balancers_all }} + {{ load_balancers_internal }}"
      firewall_zone: "{{m_private_networking_zone|default('public')}}"
    - nodejs
    - parsoid
    - parsoid-settings

- hosts: logging-servers
  become: yes
  tags: logging
  roles:
    - set-vars
    - role: meza-log
      when: docker_skip_tasks is not defined or not docker_skip_tasks

- hosts: backup-servers
  become: yes
  tags:
    - backup-cleanup
  roles:
    - set-vars
    - role: backups-cleanup
      when: backups_cleanup is defined and backups_cleanup.crontime is defined

- hosts: all:!exclude-all:!load-balancers-nonmeza:!load-balancers-nonmeza-external:!load-balancers-nonmeza-internal
  become: yes
  tags:
    - cron
    - always
  roles:
    - set-vars
    - role: cron
      when: docker_skip_tasks is not defined or not docker_skip_tasks
    - umask-unset
